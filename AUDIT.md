# TASK DESCRIPTION:
Perform a comprehensive functional audit of a Go codebase using `go-stats-generator` as the primary analysis engine to systematically identify all discrepancies between documented functionality (README.md) and actual implementation, focusing on bugs, missing features, and functional misalignments through data-driven complexity assessment.

## CONTEXT:
You are acting as an expert Go code auditor with deep knowledge of Go idioms, best practices, and common pitfalls. Your analysis will be guided by automated complexity metrics and statistical patterns generated by `go-stats-generator` to ensure comprehensive coverage of potential issues. The audit must be thorough, systematic, and provide actionable findings without modifying the codebase. You will use `go-stats-generator`'s advanced metrics to prioritize review areas and validate findings.

## PREREQUISITES:
Install and configure `go-stats-generator` for comprehensive complexity analysis and improvement tracking:

### Installation:
```bash
go install github.com/opd-ai/go-stats-generator@latest
```

## INSTRUCTIONS:

### Phase 1: Statistical Baseline Generation
1. **Generate Comprehensive Analysis Report**
    ```bash
    go-stats-generator analyze . --format json --output full-report.json --include-patterns --include-complexity --include-dependencies
    ```
    - Extract function complexity distribution and outliers (>30 lines, >10 cyclomatic complexity)
    - Analyze struct member categorization for architectural patterns
    - Review design pattern detection results for consistency violations
    - Map dependency relationships and circular dependencies
    - Identify high-risk functions based on complexity scoring

2. **Create Risk Assessment Matrix**
    - Parse JSON output to categorize functions by risk level:
      * **HIGH RISK**: Functions >50 lines OR cyclomatic complexity >15 OR >7 parameters
      * **MEDIUM RISK**: Functions 31-50 lines OR complexity 11-15 OR pattern violations
      * **LOW RISK**: Functions meeting coding standards but in critical paths
    - Generate dependency graph to identify architectural bottlenecks
    - Flag orphaned functions and unreachable code patterns

### Phase 2: Documentation-Driven Feature Mapping
3. **Requirements Extraction and Mapping**
    - Parse README.md for all functional requirements and documented behaviors
    - Cross-reference documented features with actual function signatures from go-stats-generator output
    - Identify missing implementations using pattern analysis (documented but no matching code patterns)
    - Map complex features to high-complexity functions for detailed review

### Phase 3: Risk-Prioritized Code Audit
4. **Systematic Analysis Using Statistical Guidance**
    - **Priority 1**: HIGH RISK functions (complexity outliers most likely to contain bugs)
    - **Priority 2**: Critical path functions handling core documented features
    - **Priority 3**: MEDIUM RISK functions and integration points
    - **Priority 4**: LOW RISK functions for completeness
    
    For each function:
    - Use complexity metrics to guide review depth and focus areas
    - Verify behavior against documented specifications
    - Test boundary conditions based on parameter complexity analysis
    - Check error handling patterns using go-stats-generator's pattern detection

5. **Automated Pattern Validation**
    - Use struct member categorization to verify architectural consistency
    - Cross-reference design pattern detection with documented architecture
    - Validate concurrent operation safety in functions with channel/goroutine patterns
    - Check resource management patterns identified by go-stats-generator

### Phase 4: Statistical Correlation Analysis
6. **Issue Categorization with Quantitative Backing**
    Classify findings with statistical support:
    - **CRITICAL BUG**: Functional failures in high-complexity code (>complexity threshold)
    - **FUNCTIONAL MISMATCH**: Implementation differs from docs + complexity analysis confirms risk
    - **MISSING FEATURE**: Documented but absent from pattern analysis
    - **COMPLEXITY DEBT**: Functions exceeding statistical norms requiring refactoring
    - **ARCHITECTURAL VIOLATION**: Pattern detection reveals design inconsistencies
    - **PERFORMANCE RISK**: Inefficient patterns in high-traffic code paths

## ENHANCED FORMATTING REQUIREMENTS:

### STATISTICAL AUDIT SUMMARY
```
Total Functions Analyzed: [N]
High Risk Functions: [N] (>[threshold] complexity)
Medium Risk Functions: [N] 
Critical Issues Found: [N]
Complexity Violations: [N]
Pattern Violations: [N]
Missing Features: [N]
Overall Risk Score: [calculated from complexity distribution]
```

### GO-STATS-GENERATOR INSIGHTS
```
Key Complexity Metrics:
- Average Function Length: [N] lines
- Functions >30 lines: [N] ([%])
- High Cyclomatic Complexity (>10): [N] functions
- Struct Complexity Distribution: [breakdown]
- Design Patterns Detected: [list]
- Architectural Violations: [count]

Priority Review Areas (Top 10 highest complexity):
1. [function] - Complexity: [score], Lines: [N]
2. [function] - Complexity: [score], Lines: [N]
[...]
```

### FINDINGS (Risk-Prioritized)
[Each finding in priority order with statistical backing]

### [CATEGORY]: [Issue Title] - Risk Score: [calculated]
**File:** [filename.go:line_numbers]
**Severity:** [High/Medium/Low] (based on complexity + impact)
**Complexity Metrics:** 
- Function Length: [N] lines (threshold: 30)
- Cyclomatic Complexity: [N] (threshold: 10)
- Parameter Count: [N] (threshold: 5)
- Risk Factors: [specific patterns detected]
**Statistical Context:** [How go-stats-generator metrics identified this as high-priority]
**Description:** [Issue explanation with quantitative backing]
**Expected vs Actual:** [Documentation comparison]
**Impact Assessment:** [Risk calculation based on complexity + usage patterns]
**Validation Method:** [How complexity analysis confirmed the issue]
**Code Reference:**
```go
[Annotated code snippet with complexity indicators]
```

## QUALITY VALIDATION CHECKLIST:
- [ ] Full go-stats-generator analysis completed and parsed
- [ ] Risk matrix generated from complexity metrics
- [ ] All high-risk functions reviewed first
- [ ] Findings correlate with statistical analysis
- [ ] Priority order based on quantitative risk assessment
- [ ] Missing features identified through pattern analysis gaps
- [ ] Architectural issues validated through design pattern detection
- [ ] Statistical summary provides actionable metrics for development team

## EXAMPLE ENHANCED FINDING:

### CRITICAL BUG: Buffer Overflow in High-Complexity Parser - Risk Score: 9.2/10
**File:** internal/parser/tokenizer.go:89-145
**Severity:** High (Complexity: Critical + Impact: High)
**Complexity Metrics:**
- Function Length: 57 lines (exceeds 30-line threshold by 90%)
- Cyclomatic Complexity: 18 (exceeds threshold by 80%)
- Parameter Count: 8 (exceeds 5-parameter limit)
- Risk Factors: Complex branching + buffer operations + no bounds checking
**Statistical Context:** Identified as #2 highest risk function by go-stats-generator complexity scoring. Pattern analysis detected unsafe buffer operations without validation patterns found in similar functions.
**Description:** The parseComplexToken function combines high complexity metrics with buffer manipulation, creating high probability of overflow conditions
**Expected vs Actual:** Documentation specifies safe token parsing with error handling for invalid inputs
**Impact Assessment:** Critical - High complexity + buffer operations + lack of bounds checking = high crash probability
**Validation Method:** Complexity analysis flagged function for review; manual inspection confirmed buffer overflow vulnerability
**Code Reference:**
```go
func parseComplexToken(input []byte, offset int, delims []string, opts *ParseOptions, 
                             ctx *Context, validators []Validator, transforms []Transform, debug bool) (*Token, error) {
     // 57 lines of complex branching logic - flagged by complexity analysis
     buffer := make([]byte, len(input)) // No bounds checking despite high complexity warning
     for i := offset; i < len(input); i++ {
          buffer[i-offset] = input[i] // Potential overflow - confirmed during complexity-guided review
          // 50+ more lines of complex logic with multiple nested conditions
     }
}
```